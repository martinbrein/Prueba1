{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "855vCylCRde_"
   },
   "source": [
    "# Regularización\n",
    "\n",
    "Hasta ahora, cuando ajustamos modelos lineales, seleccionamos el modelo que minimizaba el error cuadrático.\n",
    "Para un modelo de la forma\n",
    "$$y_i = f(x_i) + e_i$$\n",
    "minimizamos la suma\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}$$\n",
    "Este es un ejemplo de _funcion de costo_: una función que mide el \"costo\" de los errores de las predicciones de un modelo. Para aplicar la técnica de regularización, modificamos la función de costo, agregando un término que penaliza los modelos por su complejidad. Por ejemplo, podríamos tener una nueva función de costo de la forma:\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2 + \\alpha \\theta_i}$$\n",
    "donde el vector $\\theta$ corresponde a los parámetros de nuestro modelo y $\\alpha$ es un hiper parámetro que controla cuán fuerte es la penalización. Un mayor valor de $\\alpha$ significa una mayor penalización, ya que aumenta el costo, que es lo que buscamos minimizar.\n",
    "\n",
    "Veamos un ejemplo clásico de  ajustar un polinomio a un set de datos pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6n5ZHryRdfV",
    "outputId": "73cb8f80-9341-43f5-fb4a-84c48467bcba"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "plt.rcParams['figure.figsize'] = 6,6\n",
    "\n",
    "random.seed(5)\n",
    "\n",
    "# Generamos un dataset de ejemplo\n",
    "def generate_data():\n",
    "    xs = np.arange(-3, 3, 1)\n",
    "    data = [(2 * x - 3 * random.random(), (x - 3 * random.random()) * (x + random.random())) for x in xs]\n",
    "    data.sort()\n",
    "    xs = [x for (x, y) in data]\n",
    "    ys = [y for (x, y) in data]\n",
    "    \n",
    "    return xs, ys\n",
    "\n",
    "xs, ys = generate_data()\n",
    "plt.scatter(xs, ys)\n",
    "plt.title(\"Datos generados\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4qSZhkyRdf1"
   },
   "source": [
    "Intentemos ahora ajustar un modelo. Si intentamos ajustar un polinomio de grado 4, obtendremos lo que llamamos un modelo sobreajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vander(xs, 4), ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 874,
     "status": "error",
     "timestamp": 1535732561367,
     "user": {
      "displayName": "Pablo Roccatagliata",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "115515358344732162090"
     },
     "user_tz": 180
    },
    "id": "4gfDmWBYRdf1",
    "outputId": "20de5bc8-2e62-406d-d390-cdd085ce78c0"
   },
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "# Esta funcion de numpy genera la matriz polinomica a partir de una serie de valores\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "\n",
    "model = lm.fit(X, y)\n",
    "predictions = lm.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "\n",
    "\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "print (\"r^2:\", model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nq1fWTutRdf6"
   },
   "source": [
    "Si aplicamos este mismo modelo a una nueva muestra de datos generados por la misma funcion, veremos que no ajusta muy bien. Mejor dicho, vemos que ajusta notablemente peor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0lhXbw4Rdf8",
    "outputId": "9c4e11b8-9814-4407-e783-24562c3d199d"
   },
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "\n",
    "xs2, ys2 = generate_data()\n",
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = lm.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos #2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "print (\"r^2:\", model.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los coeficientes de la regresión lineal:    \n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SljrTKA-RdgC"
   },
   "source": [
    "#  Ridge Regression\n",
    "Usemos scikit-learn para ajustar una regresión con regularización, como la que describimos en el inicio del notebook. En particular, comenzaremos por la  _ridge regression_ en inglés. ¿Hace falta normalizar los features en este caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGqwhURqRdgE",
    "outputId": "47e1b6f2-ae7f-4485-f341-e9623b5066e1"
   },
   "outputs": [],
   "source": [
    "rlm = linear_model.Ridge(alpha=0.5, normalize=True)\n",
    "\n",
    "# Ajustamos nuevamente, esta vez con regularizacion\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "\n",
    "ridge_model = rlm.fit(X, y)\n",
    "predictions = ridge_model.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "print (\"r^2:\", ridge_model.score(X, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7ZqKmU7RdgM",
    "outputId": "5a2cdbe0-e2a2-4dd4-bff0-e0682782d02a"
   },
   "outputs": [],
   "source": [
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = ridge_model.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos#2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "\n",
    "print (\"r^2:\", ridge_model.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los coeficientes de la regresión Ridge:    \n",
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EiRnup0RdgS"
   },
   "source": [
    "Deberían observar que el ajuste de la _ridge regression_ empeoró un poco (es decir, no fue tan bueno) en muestra #1. En cambio, la mejora fue notablemente grande en la muestra #2. Esto es porque la regularización busca prevenir el sobreajuste (overfitting).\n",
    "\n",
    "Además podemos apreciar que los coeficientes de la regresión Ridge son más chicos.\n",
    "\n",
    "Si quieren ver otro ejemplo de ridge regularization pueden leer [este ejemplo](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html) de la documentacion oficial de scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1AwajaZ0yeRu"
   },
   "source": [
    "# Lasso\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKlG9F1x-tOA"
   },
   "source": [
    "¿Hace falta normalizar los features en este caso? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 849,
     "status": "error",
     "timestamp": 1535732554047,
     "user": {
      "displayName": "Pablo Roccatagliata",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "115515358344732162090"
     },
     "user_tz": 180
    },
    "id": "VpA1dd7WyeAI",
    "outputId": "bd82f8b7-d4e8-4394-c635-248102e1b9a4"
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha=0.5, normalize=True)\n",
    "\n",
    "# Ajustamos nuevamente, esta vez con regularizacion\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "lasso_model =lasso.fit(X, y)\n",
    "predictions = lasso_model.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "\n",
    "print (\"r^2:\", lasso_model.score(X, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IW7OBQjWzJmY"
   },
   "outputs": [],
   "source": [
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = lasso_model.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos#2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "\n",
    "print (\"r^2:\", lasso_model.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los coeficientes de la regresión Lasso:    \n",
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHxsSjo_zqCN"
   },
   "source": [
    "# ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxxkb8DS-0Eu"
   },
   "source": [
    "¿Hace falta normalizar los features en este caso? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2DM25fazzY4"
   },
   "outputs": [],
   "source": [
    "elastic_net = linear_model.ElasticNet(alpha=0.5, normalize=True)\n",
    "\n",
    "# Ajustamos nuevamente, esta vez con regularizacion\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "\n",
    "elastic_net.fit(X, y)\n",
    "predictions = elastic_net.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "\n",
    "print (\"r^2:\", elastic_net.score(X, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sw_Ce5I3z0Qt"
   },
   "outputs": [],
   "source": [
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = elastic_net.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos#2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "print (\"r^2:\", elastic_net.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los coeficientes de la regresión ElasticNet:\n",
    "elastic_net.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9LMivs9RdgU"
   },
   "source": [
    "# Optimizando el hiper parámetro $\\alpha$ \n",
    "\n",
    "Habrán notado que en el ejemplo seteamos el _hiperparámetro_ $\\alpha$ en 10. Esta fue una decisión arbitraria, y existen maneras de determinar este valor empíricamente.\n",
    "\n",
    "En general, debemos decidir la forma de elegir el parámetro $\\alpha$ y existen formas automáticas de hacerlo. Una forma de hacerlo es con _validacion-cruzada_ o _cross-validation_ en inglés. Para esta práctica guiada, exploremos el ridge model que viene con funcionalidad para hacer [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29). \n",
    "\n",
    "Típicamente, el cross-validation funciona separando nuestro dataset entre datos para ajustar el modelo (training set) y datos para evaluar el modelo (testing set).\n",
    "\n",
    "En este caso, el modelo que generamos con `RidgeCV` de scikit-learn, automáticamente prueba diferentes valores de $\\alpha$. Ejecutá el siguiente código más de una vez. Deberías ver que elije diferentes valores de $\\alpha$, ya que hace diferentes separaciones de los datos cada vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HpnZLJFGRdgW",
    "outputId": "825615f9-21ac-4c03-ce6d-d3d79fa95693"
   },
   "outputs": [],
   "source": [
    "rlmcv = linear_model.RidgeCV(alphas=np.linspace(0.01,100, 1000), cv=5, normalize=True)\n",
    "xs, ys = generate_data()\n",
    "\n",
    "# Ajustamos nuevamente nuestro modelo, esta vez con RidgeCV\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "rlmcv.fit(X, y)\n",
    "predictions = rlmcv.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "plt.show()\n",
    "print (\"r^2:\", rlmcv.score(X, ys))\n",
    "print (\"alpha:\", rlmcv.alpha_)\n",
    "\n",
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = rlmcv.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos #2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "plt.show()\n",
    "print (\"r^2:\", rlmcv.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv = linear_model.LassoCV(alphas=np.linspace(0.01,100, 1000), cv=5, normalize=True)\n",
    "xs, ys = generate_data()\n",
    "\n",
    "# Ajustamos nuevamente nuestro modelo, esta vez con RidgeCV\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "lassocv.fit(X, y)\n",
    "predictions = lassocv.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "plt.show()\n",
    "print (\"r^2:\", lassocv.score(X, ys))\n",
    "print (\"alpha:\", lassocv.alpha_)\n",
    "\n",
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = lassocv.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos #2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "plt.show()\n",
    "print (\"r^2:\", lassocv.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1.PRACTICA_GUIADA_Regularización_Overfitting.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
